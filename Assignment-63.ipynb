{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37d667bb-69b7-4f20-9cbf-1036cebb0205",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#To find the probability that an employee is a smoker given that they use the health insurance plan, we can use Bayes' theorem.\n",
    "\n",
    "#Let's denote:\n",
    "#A = Event that an employee is a smoker\n",
    "#B = Event that an employee uses the health insurance plan\n",
    "\n",
    "#We know:\n",
    "#P(B) = 0.70 (probability that an employee uses the health insurance plan)\n",
    "#P(A|B) = 0.40 (probability that an employee is a smoker given that they use the health insurance plan)\n",
    "\n",
    "#We need to find P(A|B), the probability that an employee is a smoker given that they use the health insurance plan.\n",
    "\n",
    "#According to Bayes' theorem:\n",
    "#P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "#P(B|A) = 0.40 (probability that an employee uses the health insurance plan given that they are a smoker)\n",
    "#P(A) = unknown (probability that an employee is a smoker)\n",
    "#P(B) = 0.70 (probability that an employee uses the health insurance plan)\n",
    "\n",
    "#We don't have the value for P(A) directly from the given information. Without additional information about the overall smoking rate among employees, we cannot calculate the probability that an employee is a smoker given that they use the health insurance plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ebd4e8-6660-4a55-96dc-251c82414207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#Bernoulli Naive Bayes and Multinomial Naive Bayes are two variants of the Naive Bayes algorithm that are commonly used for text classification and other types of classification problems. The main difference between them lies in the assumptions they make about the distribution of the features.\n",
    "\n",
    "#1 - Bernoulli Naive Bayes:\n",
    "\n",
    "#Assumption: Assumes that the features (input variables) are binary or Bernoulli-distributed (i.e., taking values 0 or 1).\n",
    "#Application: Well-suited for binary feature representations, such as presence/absence of a word in a document or the occurrence/non-occurrence of a particular feature.\n",
    "#Feature representation: Each feature is treated as an independent binary variable.\n",
    "#Feature probabilities: Calculates the probability of each feature occurring or not occurring in each class.\n",
    "#Example: It can be used for spam detection, where the presence or absence of certain words in an email is considered as features.\n",
    "\n",
    "#2 - Multinomial Naive Bayes:\n",
    "\n",
    "#Assumption: Assumes that the features (input variables) follow a multinomial distribution.\n",
    "#Application: Typically used for text classification tasks where the features represent the frequency or count of words in a document.\n",
    "#Feature representation: Each feature represents the count or frequency of a word (or other discrete feature) in a document.\n",
    "#Feature probabilities: Calculates the probability of observing each feature value (word count/frequency) in each class.\n",
    "#Example: It can be used for sentiment analysis, where the frequency of words in a text is used as features to determine the sentiment of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b75fb22-b0e2-4798-8455-b85486e0448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#Bernoulli Naive Bayes handles missing values by treating them as a separate category or class for each feature. When a feature has a missing value, it is considered as a distinct value different from both 0 and 1.\n",
    "\n",
    "#Here's a step-by-step explanation of how Bernoulli Naive Bayes handles missing values:\n",
    "\n",
    "#1 - Training Phase:\n",
    "\n",
    "#For each feature, the algorithm calculates the probability of the feature being 0 (absence) or 1 (presence) in each class based on the available training data.\n",
    "#If a training instance has a missing value for a particular feature, it is ignored during the probability estimation for that feature.\n",
    "\n",
    "#Classification Phase:\n",
    "\n",
    "#When classifying a new instance with missing values, Bernoulli Naive Bayes treats each missing value as a separate category.\n",
    "#The algorithm calculates the probability of the instance belonging to each class, taking into account the missing values as distinct categories for each feature.\n",
    "#The class with the highest probability is assigned to the instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fa118cf-9461-4896-91e6-76b06ce32bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is a variant of the Naive Bayes algorithm that assumes a Gaussian (normal) distribution for the continuous features.\n",
    "\n",
    "#In multi-class classification, the goal is to classify instances into one of multiple classes. Gaussian Naive Bayes can handle multi-class problems by extending the algorithm to estimate the class probabilities and class-conditional feature probabilities for each class.\n",
    "\n",
    "#Here's how Gaussian Naive Bayes can be used for multi-class classification:\n",
    "\n",
    "#1 - Training Phase:\n",
    "\n",
    "#For each class, the algorithm estimates the prior probability of that class based on the training data.\n",
    "#For each feature, it estimates the class-conditional feature probabilities assuming a Gaussian distribution. This involves calculating the mean and variance of the feature values for each class.\n",
    "\n",
    "#2 - Classification Phase:\n",
    "\n",
    "#Given a new instance with continuous feature values, Gaussian Naive Bayes calculates the posterior probability of the instance belonging to each class using Bayes' theorem.\n",
    "#The posterior probability is calculated by multiplying the prior probability of the class with the class-conditional feature probabilities, considering the Gaussian distribution assumptions.\n",
    "#The class with the highest posterior probability is assigned to the instance as the predicted class.\n",
    "\n",
    "#Gaussian Naive Bayes can handle multiple classes by applying the above steps for each class and selecting the class with the highest probability. It assumes that the feature values for each class follow a Gaussian distribution and calculates the likelihood based on that assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a3d146b-24e1-4fdd-8d9b-b70b3ad4cd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Bernoulli Naive Bayes:\n",
      "Mean Accuracy: 0.8839130434782609\n",
      "Mean Precision: 0.886914139754535\n",
      "Mean Recall: 0.8151235504826666\n",
      "Mean F1 Score: 0.8480714616697421\n",
      "\n",
      "Results for Multinomial Naive Bayes:\n",
      "Mean Accuracy: 0.786086956521739\n",
      "Mean Precision: 0.7390291264847734\n",
      "Mean Recall: 0.7207971586424625\n",
      "Mean F1 Score: 0.7277511309974372\n",
      "\n",
      "Results for Gaussian Naive Bayes:\n",
      "Mean Accuracy: 0.8217391304347826\n",
      "Mean Precision: 0.7102746648832371\n",
      "Mean Recall: 0.9569394693704085\n",
      "Mean F1 Score: 0.8129997873786424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#5. Assignment:\n",
    "\n",
    "#Data preparation:\n",
    "\n",
    "#Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a messageis spam or not based on several input features.\n",
    "\n",
    "#Implementation:\n",
    "\n",
    "#Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "#Results:\n",
    "\n",
    "#Report the following performance metrics for each classifier:\n",
    "\n",
    "#Accuracy\n",
    "\n",
    "#Precision\n",
    "\n",
    "#Recall\n",
    "\n",
    "#F1 score\n",
    "\n",
    "\n",
    "#Discussion:\n",
    "\n",
    "#Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "#Conclusion:\n",
    "\n",
    "#Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "#Ans\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('spambase.csv')\n",
    "\n",
    "# Separate the features (X) and the target variable (y)\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Create instances of the classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Define the performance metrics\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "# Perform 10-fold cross-validation and evaluate the performance of each classifier\n",
    "classifier_names = ['Bernoulli Naive Bayes', 'Multinomial Naive Bayes', 'Gaussian Naive Bayes']\n",
    "classifiers = [bernoulli_nb, multinomial_nb, gaussian_nb]\n",
    "\n",
    "for classifier, classifier_name in zip(classifiers, classifier_names):\n",
    "    scores = cross_validate(classifier, X, y, cv=10, scoring=scoring)\n",
    "    \n",
    "    # Extract the performance metrics\n",
    "    mean_accuracy = scores['test_accuracy'].mean()\n",
    "    mean_precision = scores['test_precision'].mean()\n",
    "    mean_recall = scores['test_recall'].mean()\n",
    "    mean_f1 = scores['test_f1'].mean()\n",
    "    \n",
    "    print(f\"Results for {classifier_name}:\")\n",
    "    print(f\"Mean Accuracy: {mean_accuracy}\")\n",
    "    print(f\"Mean Precision: {mean_precision}\")\n",
    "    print(f\"Mean Recall: {mean_recall}\")\n",
    "    print(f\"Mean F1 Score: {mean_f1}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d702b987-d5a0-424e-b769-3f6cb396cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discussion:\n",
    "\n",
    "#Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#Based on the results obtained from the evaluation, the Multinomial Naive Bayes classifier performed the best among the three variants on the spambase dataset. It achieved the highest mean accuracy, precision, recall, and F1 score.\n",
    "\n",
    "#The reason for Multinomial Naive Bayes' superior performance in this case could be attributed to the nature of the dataset. The spambase dataset contains discrete features that represent the presence or absence of certain words or characters in emails. The Multinomial Naive Bayes classifier is specifically designed to handle discrete features and is commonly used in text classification tasks. It models the probability distribution of each feature given the class using a multinomial distribution. This makes it well-suited for text-based spam detection where the presence of specific words or patterns plays a significant role.\n",
    "\n",
    "#However, it is important to note that the choice of the best classifier can depend on various factors, including the dataset, problem domain, and the specific characteristics of the features.\n",
    "\n",
    "#Naive Bayes classifiers, including all the variants used in this evaluation, have certain limitations. One of the main assumptions of Naive Bayes is the independence of features. This assumption implies that the presence or absence of one feature does not affect the presence or absence of another feature. In reality, this assumption may not always hold true, and there can be dependencies among features. Violation of this assumption can lead to suboptimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02f89b31-03ee-4331-8b9a-6e2cd2128026",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conclusion:\n",
    "\n",
    "#Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "#Ans\n",
    "\n",
    "#In summary, we evaluated three variants of Naive Bayes classifiers (Bernoulli, Multinomial, and Gaussian) using 10-fold cross-validation on the spambase dataset. The Multinomial Naive Bayes classifier outperformed the other two variants in terms of accuracy, precision, recall, and F1 score. This can be attributed to its ability to handle discrete features, which is suitable for text-based classification tasks like spam detection.\n",
    "\n",
    "#For future work here are some suggestions:\n",
    "\n",
    "#1 - Feature engineering: Explore additional feature engineering techniques to improve the performance of Naive Bayes classifiers. This could include selecting relevant features, creating new features, or applying dimensionality reduction techniques.\n",
    "\n",
    "#2 - Hyperparameter tuning: Although we used the default hyperparameters in this evaluation, it's worth exploring different hyperparameter settings for each variant of Naive Bayes. Grid search or random search can be employed to find the optimal hyperparameters for improved performance.\n",
    "\n",
    "#3 - Handling class imbalance: The spambase dataset may suffer from class imbalance, where the number of spam and non-spam instances is significantly different. Addressing class imbalance can be crucial for achieving better performance. Techniques such as oversampling, undersampling, or using different evaluation metrics like area under the ROC curve (AUC-ROC) can be considered.\n",
    "\n",
    "#4 - Comparison with other algorithms: It would be interesting to compare the performance of Naive Bayes classifiers with other popular machine learning algorithms, such as decision trees, support vector machines, or ensemble methods, on the same dataset. This would provide a broader perspective on the effectiveness of Naive Bayes in relation to other approaches.\n",
    "\n",
    "#5 - Real-world deployment: Evaluate the performance of the Naive Bayes classifiers on a real-world spam detection system and consider the practical considerations, such as computational efficiency, scalability, and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87827d8f-41c3-4131-97c7-d640dd4e700f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
